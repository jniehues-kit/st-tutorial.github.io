@inproceedings{di2019adapting,
 title={Adapting Transformer to end-to-end spoken language translation},
 author={Di Gangi, Mattia A and Negri, Matteo and Turchi, Marco},
 booktitle={INTERSPEECH 2019},
 pages={1133--1137},
 year={2019},
 organization={International Speech Communication Association (ISCA)},
 url={https://cris.fbk.eu/retrieve/handle/11582/319654/29817/3045.pdf}
 }

@inproceedings{jia2019leveraging,
 title={Leveraging weakly supervised data to improve end-to-end speech-to-text translation},
 author={Jia, Ye and Johnson, Melvin and Macherey, Wolfgang and Weiss, Ron J and Cao, Yuan and Chiu, Chung-Cheng and Ari, Naveen and Laurenzo, Stella and Wu, Yonghui},
 booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 pages={7180--7184},
 year={2019},
 organization={IEEE}
 }

@inproceedings{inaguma2019multilingual,
 title={Multilingual end-to-end speech translation},
 author={Inaguma, Hirofumi and Duh, Kevin and Kawahara, Tatsuya and Watanabe, Shinji},
 booktitle={2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
 pages={570--577},
 year={2019},
 organization={IEEE}
 }

@inproceedings{berard_2016,
author = {Bérard, Alexandre and Pietquin, Olivier and Servan, Christophe and Besacier, Laurent},
year = {2016},
month = dec,
address = {Barcelona, Spain},
title = {{Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation}},
booktitle = {NIPS Workshop on end-to-end learning for speech and audio processing}
}

@article{weiss2017sequence,
 title={Sequence-to-Sequence Models Can Directly Translate Foreign Speech},
 author={Weiss, Ron J and Chorowski, Jan and Jaitly, Navdeep and Wu, Yonghui and Chen, Zhifeng},
 journal={Proc. Interspeech 2017},
 pages={2625--2629},
 year={2017}
 }

@INPROCEEDINGS{Chan2016,  author={W. {Chan} and N. {Jaitly} and Q. {Le} and O. {Vinyals}},  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Listen, attend and spell: A neural network for large vocabulary conversational speech recognition},   year={2016},  volume={},  number={},  pages={4960-4964},}

@inproceedings{Vaswani2017,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, undefinedukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@ARTICLE{Kano2020,  author={T. {Kano} and S. {Sakti} and S. {Nakamura}},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={End-to-End Speech Translation With Transcoding by Multi-Task Learning for Distant Language Pairs},   year={2020},  volume={28},  number={},  pages={1342-1355},}

@article{Sperber2019,
 author = {Sperber, Matthias and Neubig, Graham and Niehues, Jan and Waibel, Alex},
 journal = {Transactions of the Association for Computational Linguistics (TACL)},
 keywords = {attentional encoder-decoder,multi-stage model,speech translation},
 title = {{Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation}},
 url = {https://arxiv.org/abs/1904.07209},
 year = {2019}
 }

@inproceedings{Waibel1991,
author = {Waibel, A. and Jain, A. N. and McNair, A. E. and Saito, H. and Hauptmann, A. G. and Tebelskis, J.},
title = {JANUS: A Speech-to-Speech Translation System Using Connectionist and Symbolic Processing Strategies},
year = {1991},
isbn = {0780300033},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The authors present JANUS, a speech-to-speech translation system that utilizes diverse processing strategies including dynamic programming, stochastic techniques, connectionist learning, and traditional AI knowledge representation approaches. JANUS translates continuously spoken English utterances into Japanese and German speech utterances. The overall system performance on a corpus of conference registration conversations is 87%. Two versions of JANUS are compared: one using a LR parser (JANUS 1) and one using a connectionist parser (JANUS 2). Performance results were mixed, with JANUS 1 deriving benefit from a tighter language model and JANUS 2 benefitting from greater flexibility.},
booktitle = {Proceedings of the Acoustics, Speech, and Signal Processing, 1991. ICASSP-91., 1991 International Conference},
pages = {793–796},
numpages = {4},
series = {ICASSP '91}
}
